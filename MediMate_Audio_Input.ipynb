{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü©∫ MediMate - Enhanced AI Medical Consultation System (with Audio Input)\n",
    "\n",
    "Welcome to the latest version of MediMate, now featuring **voice input**. You can now describe your symptoms by speaking directly into your microphone, making it easier and faster to get a medical consultation.\n",
    "\n",
    "## Key Enhancements in This Version\n",
    "\n",
    "1.  **üé§ Audio Input with Whisper API**\n",
    "    *   A new **Record Audio** button has been added to the interface.\n",
    "    *   Your recorded audio is transcribed into text using **OpenAI's Whisper API**.\n",
    "    *   The transcribed text is then sent to the AI, seamlessly integrating with the existing chat logic.\n",
    "\n",
    "2.  **‚ú® Streamlined UI**\n",
    "    *   The text and audio inputs are presented as clear alternatives.\n",
    "    *   Error handling is included for cases where the microphone is not available or the API key is missing.\n",
    "\n",
    "3.  **‚öôÔ∏è Robust Backend**\n",
    "    *   The `AIService` client is now reused for both chat completions and audio transcriptions.\n",
    "    *   The implementation is modular, with a dedicated function for transcription that feeds into the main chat handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, let's install all the necessary packages. This cell includes all dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ollama gradio>=4.0.0 requests python-dotenv openai>=1.0.0 colorama langdetect pydantic>=2.0.0 markdown fpdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import subprocess\n",
    "import logging\n",
    "from typing import List, Tuple, Generator, Dict, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(\"MediMate\")\n",
    "\n",
    "# Import third-party libraries\n",
    "try:\n",
    "    import gradio as gr\n",
    "    from dotenv import load_dotenv\n",
    "    from openai import OpenAI\n",
    "    from colorama import Fore, Style\n",
    "    from langdetect import detect\n",
    "    import markdown\n",
    "    from fpdf import FPDF\n",
    "    import ollama\n",
    "except ImportError as e:\n",
    "    logger.error(f\"‚ùå Import error: {e}\")\n",
    "    logger.warning(\"Some packages may need to be installed. Please run the pip install command above.\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Special handling for Ollama\n",
    "try:\n",
    "    import ollama\n",
    "except ImportError:\n",
    "    logger.warning(\"‚ö†Ô∏è Ollama import failed. Local models will not be available.\")\n",
    "    ollama = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Settings\n",
    "\n",
    "The `Config` class defines all configuration parameters. It now includes a setting for the directory where conversations will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration settings for MediMate\"\"\"\n",
    "    \n",
    "    # Ollama settings\n",
    "    OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "    DEFAULT_MODEL = os.getenv(\"OLLAMA_MODEL\", \"gemma:7b\")\n",
    "    \n",
    "    # OpenRouter settings\n",
    "    OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    OPENROUTER_MODEL = os.getenv(\"OPENROUTER_MODEL\", \"meta-llama/llama-3.2-9b-instruct:free\")\n",
    "    OPENROUTER_BASE_URL = os.getenv(\"OPENROUTER_BASE_URL\", \"https://openrouter.ai/api/v1\")\n",
    "    WHISPER_MODEL = os.getenv(\"WHISPER_MODEL\", \"openai/whisper-large-v3\") # Whisper model for transcription\n",
    "    \n",
    "    # App settings\n",
    "    SERVER_PORT = int(os.getenv(\"SERVER_PORT\", \"7860\"))\n",
    "    AI_METHOD = os.getenv(\"AI_METHOD\", \"auto\")\n",
    "    DEBUG_MODE = os.getenv(\"DEBUG_MODE\", \"false\").lower() == \"true\"\n",
    "    MAX_HISTORY = int(os.getenv(\"MAX_HISTORY\", \"10\"))\n",
    "    CONVERSATION_DIR = os.getenv(\"CONVERSATION_DIR\", \"consultations\")\n",
    "    \n",
    "    # Advanced features\n",
    "    ENABLE_MULTILINGUAL = os.getenv(\"ENABLE_MULTILINGUAL\", \"true\").lower() == \"true\"\n",
    "    DEFAULT_LANGUAGE = os.getenv(\"DEFAULT_LANGUAGE\", \"en\")\n",
    "    ENABLE_PDF_EXPORT = os.getenv(\"ENABLE_PDF_EXPORT\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    # Medical features\n",
    "    CLINICAL_DETAIL_LEVEL = os.getenv(\"CLINICAL_DETAIL_LEVEL\", \"high\")\n",
    "    EMERGENCY_MODE = os.getenv(\"EMERGENCY_MODE\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    @classmethod\n",
    "    def display(cls):\n",
    "        \"\"\"Display current configuration settings\"\"\"\n",
    "        logger.info(\"üîß MediMate Configuration:\")\n",
    "        logger.info(f\"   üñ•Ô∏è  Ollama: {cls.OLLAMA_HOST} ({cls.DEFAULT_MODEL})\")\n",
    "        logger.info(f\"   üåê OpenRouter: {cls.OPENROUTER_MODEL}\")\n",
    "        logger.info(f\"   üé§ Whisper Model: {cls.WHISPER_MODEL}\")\n",
    "        logger.info(f\"   ‚öôÔ∏è  Method: {cls.AI_METHOD}\")\n",
    "        logger.info(f\"   üîê API Key: {'‚úÖ Available' if cls.OPENROUTER_API_KEY else '‚ùå Missing'}\")\n",
    "        logger.info(f\"   üåç Multilingual: {'‚úÖ Enabled' if cls.ENABLE_MULTILINGUAL else '‚ùå Disabled'}\")\n",
    "        logger.info(f\"   üìÑ PDF Export: {'‚úÖ Enabled' if cls.ENABLE_PDF_EXPORT else '‚ùå Disabled'}\")\n",
    "        logger.info(f\"   üö® Emergency Mode: {'‚úÖ Enabled' if cls.EMERGENCY_MODE else '‚ùå Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Manual Configuration (for Local or Colab Use)\n",
    "\n",
    "This cell allows you to manually set your OpenRouter API key. **This is required to use the audio transcription feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: To use the audio transcription feature, you must provide your OpenRouter API key.\n",
    "# You can get a free key from https://openrouter.ai/\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# --- PASTE YOUR OPENROUTER API KEY HERE ---\n",
    "Config.OPENROUTER_API_KEY = \"\"\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# You can also override other settings if you wish:\n",
    "# Config.AI_METHOD = \"openrouter\" # Recommended for Colab\n",
    "# Config.OPENROUTER_MODEL = \"google/gemma-2-9b-it:free\"\n",
    "\n",
    "# Display the final configuration\n",
    "if not Config.OPENROUTER_API_KEY:\n",
    "    logger.warning(\"üî¥ OPENROUTER_API_KEY is not set! Audio transcription and OpenRouter models will not be available.\")\n",
    "else:\n",
    "    logger.info(\"üü¢ OPENROUTER_API_KEY has been set.\")\n",
    "    \n",
    "Config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System Prompts for Medical Consultation\n",
    "\n",
    "The `MedicalSystemPrompts` class defines the instructional prompts for the AI, guiding its behavior in different medical contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalSystemPrompts:\n",
    "    \"\"\"Collection of system prompts for different medical scenarios\"\"\"\n",
    "    \n",
    "    BASE = \"\"\"You are Dr. MediMate, an experienced physician providing direct medical consultations. You are the doctor the patient is seeing - not a referral service.\n",
    "\n",
    "Your role as the attending physician:\n",
    "\n",
    "1. **Conduct Medical Interviews**: Ask detailed follow-up questions to gather complete clinical information\n",
    "2. **Perform Clinical Assessment**: Analyze symptoms systematically and provide working diagnoses\n",
    "3. **Provide Direct Medical Care**: Give specific treatment recommendations, medications, and dosages\n",
    "4. **Order Diagnostic Tests**: Recommend appropriate lab work, imaging, or procedures\n",
    "5. **Manage Patient Care**: Provide ongoing medical management and follow-up plans\n",
    "\n",
    "Your consultation approach:\n",
    "- Act as the primary physician - YOU are the doctor they're consulting\n",
    "- Ask detailed medical history questions (onset, duration, severity, triggers, alleviating factors)\n",
    "- Inquire about associated symptoms, medical history, current medications, allergies\n",
    "- Provide differential diagnoses with reasoning\n",
    "- Give specific treatment plans including medications with exact dosages and schedules\n",
    "- Recommend diagnostic tests when clinically indicated\n",
    "- Provide clear medical advice and next steps\n",
    "- Only suggest emergency care for true medical emergencies\n",
    "\n",
    "Communication style:\n",
    "- Professional but direct medical communication\n",
    "- Ask targeted clinical questions to gather information\n",
    "- Provide confident medical assessments\n",
    "- Give specific, actionable medical recommendations\n",
    "- Explain your clinical reasoning clearly\n",
    "\n",
    "Remember: You ARE the doctor providing the consultation. Gather information, make assessments, and provide treatment recommendations like any physician would during a consultation.\"\"\"\n",
    "\n",
    "    EMERGENCY = BASE + \"\"\"\n",
    "\n",
    "IMPORTANT: You are handling a potential medical emergency. Prioritize assessment of life-threatening conditions and provide immediate guidance while recommending emergency services when appropriate. Be direct and clear in your instructions for both immediate first aid and when to call emergency services.\n",
    "\n",
    "Emergency priorities:\n",
    "- Assess ABC (Airway, Breathing, Circulation) status first\n",
    "- Provide immediate first aid instructions when applicable\n",
    "- Clearly state when emergency medical services (911/999/112) should be called\n",
    "- Give specific instructions for what to do while waiting for emergency services\n",
    "- Use direct, concise language appropriate for crisis situations\"\"\"\n",
    "\n",
    "    PEDIATRIC = BASE + \"\"\"\n",
    "\n",
    "SPECIAL CONSIDERATIONS: You are treating a pediatric patient. Adjust your approach accordingly:\n",
    "- Use age-appropriate assessment\n",
    "- Calculate medication dosages based on weight when applicable\n",
    "- Consider developmental factors in your evaluation\n",
    "- Include guidance specifically for parents/caregivers\n",
    "- Be especially vigilant about safety and emergency situations\"\"\"\n",
    "\n",
    "    CHRONIC = BASE + \"\"\"\n",
    "\n",
    "FOCUS: You are managing a patient with chronic condition(s). Consider:\n",
    "- Long-term medication management and adherence\n",
    "- Disease progression monitoring\n",
    "- Quality of life improvements\n",
    "- Coordination with specialists\n",
    "- Patient education for self-management\n",
    "- Strategies to prevent complications\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def get_prompt(cls, patient_type=\"standard\", is_emergency=False):\n",
    "        \"\"\"Get the appropriate system prompt based on context\"\"\"\n",
    "        if is_emergency:\n",
    "            return cls.EMERGENCY\n",
    "            \n",
    "        if patient_type == \"pediatric\":\n",
    "            return cls.PEDIATRIC\n",
    "        elif patient_type == \"chronic\":\n",
    "            return cls.CHRONIC\n",
    "        else:\n",
    "            return cls.BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AI Service Management\n",
    "\n",
    "The `AIService` class handles connections to AI providers (Ollama or OpenRouter) and manages chat completions, including failover logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIService:\n",
    "    \"\"\"Manages connections to AI providers and handles chat completions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.openrouter_client = None\n",
    "        self.ollama_available = False\n",
    "        self.openrouter_available = False\n",
    "        self.selected_method = None\n",
    "        self.initialize_services()\n",
    "        \n",
    "    def initialize_services(self):\n",
    "        \"\"\"Initialize and test AI service connections\"\"\"\n",
    "        logger.info(\"üîç Checking AI service connections...\")\n",
    "        \n",
    "        # Initialize OpenRouter if API key is available\n",
    "        if Config.OPENROUTER_API_KEY:\n",
    "            try:\n",
    "                self.openrouter_client = OpenAI(\n",
    "                    base_url=Config.OPENROUTER_BASE_URL,\n",
    "                    api_key=Config.OPENROUTER_API_KEY,\n",
    "                )\n",
    "                logger.info(\"‚úÖ OpenRouter client initialized\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to initialize OpenRouter client: {e}\")\n",
    "                self.openrouter_client = None\n",
    "        \n",
    "        # Check service availability\n",
    "        self.ollama_available = self._check_ollama_connection()\n",
    "        self.openrouter_available = self._check_openrouter_connection()\n",
    "        \n",
    "        # Select method based on configuration and availability\n",
    "        self._select_method()\n",
    "        \n",
    "    def _check_ollama_connection(self):\n",
    "        \"\"\"Check if Ollama service is running and accessible\"\"\"\n",
    "        if not ollama:\n",
    "            logger.warning(\"‚ö†Ô∏è Ollama module not available\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(f\"{Config.OLLAMA_HOST}/api/version\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                logger.info(\"‚úÖ Ollama is running and accessible!\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.warning(f\"‚ùå Ollama responded with status code: {response.status_code}\")\n",
    "                return False\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.warning(\"‚ùå Cannot connect to Ollama!\")\n",
    "            logger.info(\"üí° Make sure to:\")\n",
    "            logger.info(\"   1. Install Ollama from https://ollama.ai/\")\n",
    "            logger.info(\"   2. Run 'ollama serve' in terminal\")\n",
    "            logger.info(f\"   3. Download a model with 'ollama pull {Config.DEFAULT_MODEL}'\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ùå Error checking Ollama connection: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def _check_openrouter_connection(self):\n",
    "        \"\"\"Check if OpenRouter API is accessible\"\"\"\n",
    "        if not self.openrouter_client:\n",
    "            logger.warning(\"‚ùå OpenRouter client not available (missing API key)\")\n",
    "            return False\n",
    "            \n",
    "        # Try multiple free models in case one is unavailable\n",
    "        test_models = [\n",
    "            \"meta-llama/llama-3.2-9b-instruct:free\",\n",
    "            \"meta-llama/llama-3.2-3b-instruct:free\",\n",
    "            \"microsoft/phi-3-medium-128k-instruct:free\", \n",
    "            \"google/gemma-2-9b-it:free\"\n",
    "        ]\n",
    "        \n",
    "        for model in test_models:\n",
    "            try:\n",
    "                response = self.openrouter_client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "                    max_tokens=1\n",
    "                )\n",
    "                logger.info(f\"‚úÖ OpenRouter ready with model: {model}\")\n",
    "                # Update the global model if this one works and user used default\n",
    "                if Config.OPENROUTER_MODEL == os.getenv(\"OPENROUTER_MODEL\", \"meta-llama/llama-3.2-9b-instruct:free\"):\n",
    "                    Config.OPENROUTER_MODEL = model\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Model {model} failed: {str(e)[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        logger.error(\"‚ùå OpenRouter: All test models failed\")\n",
    "        return False\n",
    "        \n",
    "    def _select_method(self):\n",
    "        \"\"\"Determine which AI method to use based on config and availability\"\"\"\n",
    "        if Config.AI_METHOD == \"ollama\" and self.ollama_available:\n",
    "            self.selected_method = \"ollama\"\n",
    "            logger.info(\"üéØ Using: Ollama (Local)\")\n",
    "        elif Config.AI_METHOD == \"openrouter\" and self.openrouter_available:\n",
    "            self.selected_method = \"openrouter\"\n",
    "            logger.info(\"üéØ Using: OpenRouter (API)\")\n",
    "        elif Config.AI_METHOD == \"auto\":\n",
    "            if self.openrouter_available: # Prioritize OpenRouter in auto mode if available\n",
    "                self.selected_method = \"openrouter\"\n",
    "                logger.info(\"üéØ Auto-selected: OpenRouter (API)\")\n",
    "            elif self.ollama_available:\n",
    "                self.selected_method = \"ollama\"\n",
    "                logger.info(\"üéØ Auto-selected: Ollama (Local)\")\n",
    "            else:\n",
    "                self.selected_method = None\n",
    "                logger.error(\"‚ùå No AI services available!\")\n",
    "        else:\n",
    "            self.selected_method = None\n",
    "            logger.error(\"‚ùå Selected AI method is not available!\")\n",
    "            \n",
    "        # Additional logging for selected method\n",
    "        if self.selected_method == \"ollama\" and self.ollama_available:\n",
    "            try:\n",
    "                models = ollama.list()\n",
    "                logger.info(f\"\\nüìã Available Ollama models:\")\n",
    "                for model in models.get('models', []):\n",
    "                    logger.info(f\"   ‚Ä¢ {model.get('name', 'Unknown')}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è  Could not list Ollama models: {e}\")\n",
    "        elif self.selected_method == \"openrouter\":\n",
    "            logger.info(f\"\\nüåê Using OpenRouter model: {Config.OPENROUTER_MODEL}\")\n",
    "            \n",
    "    def get_chat_completion(self, \n",
    "                           messages: List[Dict[str, str]], \n",
    "                           stream: bool = True, \n",
    "                           temperature: float = 0.7,\n",
    "                           max_tokens: int = 1000) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Get chat completion from the selected AI service\n",
    "        \n",
    "        Args:\n",
    "            messages: List of message dictionaries with role and content\n",
    "            stream: Whether to stream the response\n",
    "            temperature: Temperature parameter for generation\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Yields:\n",
    "            Partial response strings when streaming is enabled\n",
    "        \"\"\"\n",
    "        if self.selected_method == \"ollama\" and self.ollama_available:\n",
    "            yield from self._get_ollama_completion(messages, stream, temperature, max_tokens)\n",
    "        elif self.selected_method == \"openrouter\" and self.openrouter_available:\n",
    "            yield from self._get_openrouter_completion(messages, stream, temperature, max_tokens)\n",
    "        else:\n",
    "            yield \"‚ùå No AI service is available. Please check your configuration and connections.\"\n",
    "            \n",
    "    def _get_ollama_completion(self, \n",
    "                              messages: List[Dict[str, str]], \n",
    "                              stream: bool = True,\n",
    "                              temperature: float = 0.7,\n",
    "                              max_tokens: int = 1000) -> Generator[str, None, None]:\n",
    "        \"\"\"Get completion from Ollama\"\"\"\n",
    "        try:\n",
    "            response_text = \"\"\n",
    "            for chunk in ollama.chat(\n",
    "                model=Config.DEFAULT_MODEL,\n",
    "                messages=messages,\n",
    "                stream=stream,\n",
    "                options={\n",
    "                    \"temperature\": temperature,\n",
    "                    \"num_predict\": max_tokens\n",
    "                }\n",
    "            ):\n",
    "                if 'message' in chunk and 'content' in chunk['message']:\n",
    "                    token = chunk['message']['content']\n",
    "                    response_text += token\n",
    "                    yield response_text\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Ollama error: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            yield error_msg\n",
    "            \n",
    "            # Try to fallback to OpenRouter if available\n",
    "            if self.openrouter_available:\n",
    "                fallback_msg = \"\\n\\nüí° Trying OpenRouter as backup...\"\n",
    "                logger.info(fallback_msg)\n",
    "                yield error_msg + fallback_msg\n",
    "                yield from self._get_openrouter_completion(messages, stream, temperature, max_tokens)\n",
    "            \n",
    "    def _get_openrouter_completion(self, \n",
    "                                  messages: List[Dict[str, str]], \n",
    "                                  stream: bool = True,\n",
    "                                  temperature: float = 0.7,\n",
    "                                  max_tokens: int = 1000) -> Generator[str, None, None]:\n",
    "        \"\"\"Get completion from OpenRouter\"\"\"\n",
    "        try:\n",
    "            # Get response from OpenRouter\n",
    "            response = self.openrouter_client.chat.completions.create(\n",
    "                model=Config.OPENROUTER_MODEL,\n",
    "                messages=messages,\n",
    "                stream=stream,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                extra_headers={\n",
    "                    \"HTTP-Referer\": \"http://localhost:7860\",\n",
    "                    \"X-Title\": \"MediMate Medical Assistant\",\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            response_text = \"\"\n",
    "            for chunk in response:\n",
    "                if chunk.choices and chunk.choices[0].delta.content:\n",
    "                    token = chunk.choices[0].delta.content\n",
    "                    response_text += token\n",
    "                    yield response_text\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå OpenRouter error: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            yield error_msg\n",
    "            \n",
    "            # Try to fallback to Ollama if available\n",
    "            if self.ollama_available:\n",
    "                fallback_msg = \"\\n\\nüí° Trying Ollama as backup...\"\n",
    "                logger.info(fallback_msg)\n",
    "                yield error_msg + fallback_msg\n",
    "                yield from self._get_ollama_completion(messages, stream, temperature, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Medical Consultation Logic\n",
    "\n",
    "The `MedicalConsultation` class handles the core logic. It now prepends emergency warnings to the AI response stream for a smoother UI experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalConsultation:\n",
    "    \"\"\"Handles medical consultation logic and session management\"\"\"\n",
    "    \n",
    "    def __init__(self, ai_service: AIService):\n",
    "        self.ai_service = ai_service\n",
    "        self.current_language = Config.DEFAULT_LANGUAGE\n",
    "        self.patient_type = \"standard\"\n",
    "        self.is_emergency = False\n",
    "        \n",
    "    def get_system_prompt(self):\n",
    "        \"\"\"Get the appropriate system prompt for the current consultation\"\"\"\n",
    "        return MedicalSystemPrompts.get_prompt(\n",
    "            patient_type=self.patient_type,\n",
    "            is_emergency=self.is_emergency\n",
    "        )\n",
    "        \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of the input text\"\"\"\n",
    "        if not Config.ENABLE_MULTILINGUAL:\n",
    "            return Config.DEFAULT_LANGUAGE\n",
    "            \n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return Config.DEFAULT_LANGUAGE\n",
    "            \n",
    "    def detect_emergency(self, text: str) -> bool:\n",
    "        \"\"\"Detect if the message indicates a medical emergency\"\"\"\n",
    "        if not Config.EMERGENCY_MODE:\n",
    "            return False\n",
    "            \n",
    "        # Expanded list of keywords for better detection\n",
    "        emergency_keywords = [\n",
    "            \"emergency\", \"severe pain\", \"chest pain\", \"heart attack\", \"stroke\", \n",
    "            \"can't breathe\", \"cannot breathe\", \"difficulty breathing\", \"unconscious\", \"collapsed\",\n",
    "            \"bleeding heavily\", \"suicide\", \"overdose\", \"seizure\", \"not breathing\",\n",
    "            \"choking\", \"major trauma\", \"head injury\", \"loss of consciousness\", \"am I having a stroke\"\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for keyword in emergency_keywords:\n",
    "            if keyword in text_lower:\n",
    "                logger.warning(f\"üö® Emergency keyword detected: {keyword}\")\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "    def detect_patient_type(self, text: str) -> str:\n",
    "        \"\"\"Detect the type of patient from the message\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for pediatric patient\n",
    "        pediatric_indicators = [\n",
    "            \"my child\", \"my baby\", \"my son\", \"my daughter\", \"infant\", \"toddler\",\n",
    "            \"year old\", \"month old\", \"week old\", \"pediatric\", \"children's\"\n",
    "        ]\n",
    "        for indicator in pediatric_indicators:\n",
    "            if indicator in text_lower:\n",
    "                return \"pediatric\"\n",
    "                \n",
    "        # Check for chronic condition patient\n",
    "        chronic_indicators = [\n",
    "            \"chronic\", \"long-term\", \"ongoing\", \"years\", \"diabetes\", \"hypertension\",\n",
    "            \"high blood pressure\", \"asthma\", \"copd\", \"arthritis\", \"long covid\"\n",
    "        ]\n",
    "        for indicator in chronic_indicators:\n",
    "            if indicator in text_lower:\n",
    "                return \"chronic\"\n",
    "                \n",
    "        return \"standard\"\n",
    "        \n",
    "    def process_user_message(self, message: str, history: List[Tuple[str, str]]) -> Generator[str, None, None]:\n",
    "        \"\"\"\n",
    "        Process a user message and generate a response\n",
    "        \n",
    "        Args:\n",
    "            message: The user's message\n",
    "            history: The conversation history\n",
    "            \n",
    "        Yields:\n",
    "            The assistant's response, chunk by chunk\n",
    "        \"\"\"\n",
    "        # Detect language, emergency status, and patient type for the current message\n",
    "        self.current_language = self.detect_language(message)\n",
    "        self.is_emergency = self.detect_emergency(message)\n",
    "        # Don't override patient type if it was manually set\n",
    "        if self.patient_type == \"standard\":\n",
    "            self.patient_type = self.detect_patient_type(message)\n",
    "        \n",
    "        # Build conversation context\n",
    "        conversation = [{\"role\": \"system\", \"content\": self.get_system_prompt()}]\n",
    "        \n",
    "        # Add chat history (limited to prevent context overflow)\n",
    "        for user_msg, assistant_msg in history[-Config.MAX_HISTORY:]:\n",
    "            conversation.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            # Remove any previous emergency prefixes from history to not confuse the model\n",
    "            if assistant_msg.startswith(\"‚ö†Ô∏è **POTENTIAL EMERGENCY DETECTED**\"):\n",
    "                assistant_msg = assistant_msg.split(\"\\n\\n\", 1)[-1]\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        \n",
    "        # Add current message\n",
    "        conversation.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Get response stream from AI service\n",
    "        response_generator = self.ai_service.get_chat_completion(conversation)\n",
    "        \n",
    "        # Prepend emergency prefix to the stream if needed\n",
    "        if self.is_emergency:\n",
    "            emergency_prefix = \"‚ö†Ô∏è **POTENTIAL EMERGENCY DETECTED** - Providing urgent assessment. **If this is a real emergency, call your local emergency services immediately.**\\n\\n\"\n",
    "            \n",
    "            # Yield the prefix prepended to each chunk from the AI\n",
    "            try:\n",
    "                first_chunk = next(response_generator)\n",
    "                yield emergency_prefix + first_chunk\n",
    "                for chunk in response_generator:\n",
    "                    yield emergency_prefix + chunk\n",
    "            except StopIteration:\n",
    "                # If the generator is empty, just yield the prefix\n",
    "                yield emergency_prefix\n",
    "        else:\n",
    "            # If not an emergency, just yield the original generator\n",
    "            yield from response_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PDF Export Functionality\n",
    "\n",
    "The `PDFExporter` is updated to accept more metadata and provide a more informative report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExporter:\n",
    "    \"\"\"Handles exporting conversation to PDF\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def export_to_pdf(history: List[Tuple[str, str]], patient_type: str, is_emergency: bool, filename: str = None) -> str:\n",
    "        \"\"\"Export conversation history to a PDF file. Returns the file path.\"\"\"\n",
    "        if not Config.ENABLE_PDF_EXPORT:\n",
    "            return \"PDF export is disabled in configuration.\"\n",
    "            \n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            if not os.path.exists(Config.CONVERSATION_DIR):\n",
    "                os.makedirs(Config.CONVERSATION_DIR)\n",
    "\n",
    "            # Create PDF object\n",
    "            pdf = FPDF()\n",
    "            pdf.add_page()\n",
    "            \n",
    "            # Set up fonts\n",
    "            pdf.set_font(\"Arial\", \"B\", 16)\n",
    "            pdf.cell(0, 10, \"MediMate Medical Consultation Record\", ln=True, align=\"C\")\n",
    "            pdf.set_font(\"Arial\", \"\", 12)\n",
    "            \n",
    "            # Add metadata\n",
    "            pdf.cell(0, 10, f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", ln=True)\n",
    "            pdf.cell(0, 10, f\"Patient Profile: {patient_type.capitalize()}\", ln=True)\n",
    "            if is_emergency:\n",
    "                pdf.set_text_color(255, 0, 0)\n",
    "                pdf.cell(0, 10, \"Status: Potential Emergency Detected in Last Message\", ln=True)\n",
    "                pdf.set_text_color(0, 0, 0)\n",
    "            pdf.ln(5)\n",
    "            \n",
    "            # Add conversation\n",
    "            pdf.set_font(\"Arial\", \"B\", 12)\n",
    "            pdf.cell(0, 10, \"Consultation Transcript:\", ln=True)\n",
    "            pdf.set_font(\"Arial\", \"\", 10)\n",
    "            \n",
    "            for i, (user_msg, assistant_msg) in enumerate(history):\n",
    "                pdf.set_font(\"Arial\", \"B\", 10)\n",
    "                pdf.multi_cell(0, 5, f\"Patient:\")\n",
    "                pdf.set_font(\"Arial\", \"\", 10)\n",
    "                pdf.multi_cell(0, 5, user_msg)\n",
    "                pdf.ln(3)\n",
    "                \n",
    "                pdf.set_font(\"Arial\", \"B\", 10)\n",
    "                pdf.multi_cell(0, 5, f\"Dr. MediMate:\")\n",
    "                pdf.set_font(\"Arial\", \"\", 10)\n",
    "                pdf.multi_cell(0, 5, assistant_msg)\n",
    "                pdf.ln(8)\n",
    "            \n",
    "            # Add disclaimer\n",
    "            pdf.ln(5)\n",
    "            pdf.set_font(\"Arial\", \"I\", 8)\n",
    "            pdf.multi_cell(0, 5, \"Disclaimer: This consultation record is provided for informational purposes only and does not constitute medical advice. Please consult with a qualified healthcare professional for diagnosis and treatment of medical conditions.\")\n",
    "            \n",
    "            # Generate filename if not provided\n",
    "            if not filename:\n",
    "                filename = os.path.join(\n",
    "                    Config.CONVERSATION_DIR,\n",
    "                    f\"MediMate_Consultation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n",
    "                )\n",
    "                \n",
    "            # Save PDF\n",
    "            pdf.output(filename)\n",
    "            logger.info(f\"‚úÖ Consultation exported to {filename}\")\n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error exporting to PDF: {e}\")\n",
    "            return f\"‚ùå Failed to export conversation: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. User Interface with Gradio\n",
    "\n",
    "The `MediMateUI` class is updated with a `gr.Audio` component and the logic to handle transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediMateUI:\n",
    "    \"\"\"Handles the Gradio UI for MediMate\"\"\"\n",
    "    \n",
    "    def __init__(self, consultation: MedicalConsultation):\n",
    "        self.consultation = consultation\n",
    "        self.interface = None\n",
    "        \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create and configure the Gradio interface\"\"\"\n",
    "        custom_css = \"\"\"\n",
    "        .gradio-container { max-width: 850px !important; margin: auto !important; }\n",
    "        .disclaimer { background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 5px; padding: 15px; margin: 10px 0; color: #856404; }\n",
    "        .emergency-notice { background-color: #f8d7da; border: 1px solid #f5c6cb; border-radius: 5px; padding: 15px; margin: 10px 0; color: #721c24; font-weight: bold; }\n",
    "        \"\"\"\n",
    "        \n",
    "        ai_status = self.consultation.ai_service.selected_method\n",
    "        theme = gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"indigo\")\n",
    "        \n",
    "        feature_desc = self._build_feature_description()\n",
    "        examples = [\n",
    "            \"I have sharp chest pain for 2 hours, worse when breathing deeply. What do you think it is?\",\n",
    "            \"Severe headaches every morning for 3 weeks, pounding pain. What tests should I get?\",\n",
    "            \"My 5-year-old has 101¬∞F fever, cough, runny nose for 2 days. What medicine should I give?\",\n",
    "            \"I've been diagnosed with type 2 diabetes. What diet should I follow?\",\n",
    "        ]\n",
    "        \n",
    "        with gr.Blocks(theme=theme, css=custom_css, title=\"MediMate\") as interface:\n",
    "            gr.Markdown(\"<h1>ü©∫ MediMate - Professional AI Medical Consultation</h1>\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.Markdown(feature_desc)\n",
    "                with gr.Column(scale=1):\n",
    "                    gr.Markdown(\"### üîç Current Configuration\")\n",
    "                    with gr.Group():\n",
    "                        gr.Markdown(f\"\"\"\n",
    "                        - üéØ **AI Engine**: {ai_status.upper() if ai_status else \"‚ùå UNAVAILABLE\"}\n",
    "                        - üé§ **Whisper**: {'‚úÖ Enabled' if self.consultation.ai_service.openrouter_client else '‚ùå Disabled'}\n",
    "                        - üìÑ **PDF Export**: {\"‚úÖ Enabled\" if Config.ENABLE_PDF_EXPORT else \"‚ùå Disabled\"}\n",
    "                        - üö® **Emergency Detection**: {\"‚úÖ Enabled\" if Config.EMERGENCY_MODE else \"‚ùå Disabled\"}\n",
    "                        \"\"\")\n",
    "            \n",
    "            gr.Markdown(\"<div class='emergency-notice'>‚ö†Ô∏è <strong>IMPORTANT MEDICAL DISCLAIMER</strong>: This AI is for informational purposes only. In case of a real medical emergency, call your local emergency services immediately (e.g., 911, 999, 112).</div>\")\n",
    "            \n",
    "            chatbot = gr.Chatbot(label=\"Medical Consultation\", height=500, show_copy_button=True, bubble_full_width=False)\n",
    "            \n",
    "            # Input area with Text and Audio options\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(placeholder=\"Type your symptoms here...\", label=\"Your Message (Text Input)\", lines=3, scale=7)\n",
    "                    audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"Or Record Your Message (Audio Input)\", scale=3)\n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Send Message\", variant=\"primary\")\n",
    "\n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"Start New Consultation\", variant=\"secondary\")\n",
    "                \n",
    "            with gr.Accordion(\"Advanced Options & Session Management\", open=False):\n",
    "                patient_type = gr.Radio([\"standard\", \"pediatric\", \"chronic\"], label=\"Patient Type\", value=\"standard\", info=\"Select patient type for specialized advice.\")\n",
    "                with gr.Row():\n",
    "                    if Config.ENABLE_PDF_EXPORT:\n",
    "                        export_pdf_btn = gr.Button(\"Export to PDF üìÑ\", variant=\"secondary\")\n",
    "                    save_chat_btn = gr.Button(\"Save Chat üíæ\", variant=\"secondary\")\n",
    "                    load_chat_btn = gr.UploadButton(\"Load Chat üìÇ\", file_types=[\".json\"])\n",
    "                \n",
    "                download_file = gr.File(label=\"Download Your Exported File\", interactive=False)\n",
    "\n",
    "            gr.Examples(examples=examples, inputs=msg, label=\"Example Medical Questions\")\n",
    "            \n",
    "            # Event handlers\n",
    "            submit_btn.click(self._chat_wrapper, [msg, chatbot], [msg, chatbot])\n",
    "            audio_input.stop_recording(self._transcribe_and_chat, [audio_input, chatbot], [msg, chatbot, audio_input])\n",
    "\n",
    "            clear_btn.click(lambda: (None, None, \"standard\", None), None, [msg, chatbot, patient_type, audio_input])\n",
    "            patient_type.change(self._update_patient_type, patient_type, None)\n",
    "            \n",
    "            if Config.ENABLE_PDF_EXPORT:\n",
    "                export_pdf_btn.click(self._export_consultation, [chatbot], [download_file])\n",
    "            \n",
    "            save_chat_btn.click(self._save_consultation, [chatbot], [download_file])\n",
    "            load_chat_btn.upload(self._load_consultation, [load_chat_btn], [chatbot])\n",
    "                \n",
    "        self.interface = interface\n",
    "        return interface\n",
    "    \n",
    "    def _build_feature_description(self):\n",
    "        ai_status = self.consultation.ai_service.selected_method\n",
    "        status_color = \"#e3f2fd\" if ai_status else \"#f8d7da\"\n",
    "        status_text = \"üñ•Ô∏è Running locally via Ollama\" if ai_status == 'ollama' else \"üåê Using OpenRouter API\" if ai_status == 'openrouter' else \"‚ùå No AI service available\"\n",
    "        return f\"\"\"\n",
    "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #007bff;'>\n",
    "        <h3>üë®‚Äç‚öïÔ∏è Welcome to MediMate</h3>\n",
    "        <p>Describe your symptoms by typing or recording audio for a professional medical assessment.</p></div>\n",
    "        <div style='background-color: {status_color}; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
    "        <p><b>AI Service Status:</b> {status_text}</p></div>\n",
    "        \"\"\"\n",
    "    \n",
    "    def _transcribe_and_chat(self, audio_path, history):\n",
    "        \"\"\"Transcribes audio and then passes the text to the chat wrapper.\"\"\"\n",
    "        if not audio_path:\n",
    "            return \"\", history, None\n",
    "            \n",
    "        client = self.consultation.ai_service.openrouter_client\n",
    "        if not client:\n",
    "            gr.Warning(\"Audio transcription requires an OpenRouter API key. Please configure it and restart.\")\n",
    "            return \"\", history, None\n",
    "\n",
    "        gr.Info(\"Transcribing audio... Please wait.\")\n",
    "        try:\n",
    "            with open(audio_path, \"rb\") as audio_file:\n",
    "                transcription = client.audio.transcriptions.create(\n",
    "                    model=Config.WHISPER_MODEL,\n",
    "                    file=audio_file\n",
    "                )\n",
    "            transcribed_text = transcription.text\n",
    "            gr.Info(\"Transcription successful! Sending to AI...\")\n",
    "\n",
    "            # Now, call the chat wrapper with the transcribed text\n",
    "            # We need to handle the generator returned by _chat_wrapper\n",
    "            final_msg, final_history = \"\", history\n",
    "            for msg_chunk, history_chunk in self._chat_wrapper(transcribed_text, history):\n",
    "                final_msg, final_history = msg_chunk, history_chunk\n",
    "                yield final_msg, final_history, None # Update UI progressively\n",
    "\n",
    "            yield \"\", final_history, None # Final update to clear textbox\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error during audio transcription: {e}\")\n",
    "            gr.Error(f\"Audio transcription failed: {e}\")\n",
    "            yield \"\", history, None\n",
    "\n",
    "    def _chat_wrapper(self, message, history):\n",
    "        if not message or message.strip() == \"\":\n",
    "            gr.Warning(\"Please enter a message.\")\n",
    "            yield \"\", history\n",
    "            return\n",
    "            \n",
    "        history = history or []\n",
    "        history.append((message, \"\"))\n",
    "        \n",
    "        response_generator = self.consultation.process_user_message(message, history[:-1])\n",
    "        \n",
    "        for chunk in response_generator:\n",
    "            history[-1] = (message, chunk)\n",
    "            yield \"\", history\n",
    "            \n",
    "    def _update_patient_type(self, patient_type):\n",
    "        self.consultation.patient_type = patient_type\n",
    "        gr.Info(f\"Patient type set to: {patient_type.capitalize()}\")\n",
    "        \n",
    "    def _export_consultation(self, history):\n",
    "        if not history:\n",
    "            gr.Warning(\"No consultation to export.\")\n",
    "            return None\n",
    "        \n",
    "        filepath = PDFExporter.export_to_pdf(\n",
    "            history, \n",
    "            self.consultation.patient_type, \n",
    "            self.consultation.is_emergency\n",
    "        )\n",
    "        \n",
    "        if \"‚ùå\" in filepath:\n",
    "            gr.Error(filepath)\n",
    "            return None\n",
    "        \n",
    "        gr.Info(\"PDF export successful!\")\n",
    "        return filepath\n",
    "\n",
    "    def _save_consultation(self, history):\n",
    "        if not history:\n",
    "            gr.Warning(\"No consultation to save.\")\n",
    "            return None\n",
    "        \n",
    "        if not os.path.exists(Config.CONVERSATION_DIR):\n",
    "            os.makedirs(Config.CONVERSATION_DIR)\n",
    "            \n",
    "        filename = os.path.join(Config.CONVERSATION_DIR, f\"MediMate_Chat_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "            \n",
    "        gr.Info(f\"Conversation saved!\")\n",
    "        return filename\n",
    "\n",
    "    def _load_consultation(self, file):\n",
    "        if not file:\n",
    "            gr.Warning(\"File upload failed.\")\n",
    "            return None\n",
    "        try:\n",
    "            with open(file.name, 'r') as f:\n",
    "                history = json.load(f)\n",
    "            gr.Info(f\"Conversation loaded from {os.path.basename(file.name)}\")\n",
    "            return history\n",
    "        except Exception as e:\n",
    "            gr.Error(f\"Failed to load or parse JSON file: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def launch(self, **kwargs):\n",
    "        if not self.interface:\n",
    "            self.create_interface()\n",
    "        \n",
    "        # Automatically enable sharing if in Google Colab\n",
    "        is_colab = 'google.colab' in sys.modules\n",
    "        share_ui = kwargs.get('share', is_colab)\n",
    "        if is_colab:\n",
    "            logger.info(\"‚úÖ Running in Google Colab. Public link will be generated.\")\n",
    "            \n",
    "        try:\n",
    "            self.interface.launch(\n",
    "                server_name=kwargs.get(\"server_name\", \"0.0.0.0\"),\n",
    "                server_port=kwargs.get(\"server_port\", Config.SERVER_PORT),\n",
    "                share=share_ui,\n",
    "                quiet=kwargs.get(\"quiet\", False),\n",
    "                show_error=kwargs.get(\"show_error\", True),\n",
    "                inbrowser=kwargs.get(\"inbrowser\", True)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error launching interface: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Execution Function\n",
    "\n",
    "This function initializes and runs the MediMate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_medimate():\n",
    "    \"\"\"Main function to run MediMate\"\"\"\n",
    "    logger.info(\"üöÄ Starting MediMate - Enhanced AI Medical Consultation System\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Initialize AI service\n",
    "    ai_service = AIService()\n",
    "    \n",
    "    if not ai_service.selected_method:\n",
    "        logger.error(\"‚ùå MediMate cannot start - No AI backend available\")\n",
    "        logger.info(\"\\nüîß Setup Options:\")\n",
    "        logger.info(\"   1. For Colab/API use: Set your OPENROUTER_API_KEY in cell #3.\")\n",
    "        logger.info(\"   2. For local use: Install and run Ollama.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize consultation manager\n",
    "    consultation = MedicalConsultation(ai_service)\n",
    "    \n",
    "    # Create and launch UI\n",
    "    ui = MediMateUI(consultation)\n",
    "    ui.create_interface()\n",
    "    \n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"üë®‚Äç‚öïÔ∏è Dr. MediMate is ready for consultations...\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    # Launch the interface\n",
    "    ui.launch()\n",
    "    \n",
    "    return ui.interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Launch MediMate\n",
    "\n",
    "Execute the cell below to start the application. If you are in Google Colab, a public link will be generated to access the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MediMate\n",
    "# This condition ensures it runs automatically in Colab but not if imported as a module.\n",
    "if __name__ == \"__main__\" and 'google.colab' in sys.modules:\n",
    "    run_medimate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
