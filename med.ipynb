{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "intro_markdown_final_v11",
      "metadata": {},
      "source": [
       "# ü©∫ MediMate - AI Medical Consultation (Final Self-Hosted Version)\n",
       "\n",
       "This notebook contains the complete, self-contained backend for the MediMate API. It installs and runs the **Ollama server and a language model directly within the Colab environment**, removing the need for any external API keys. All original features and code have been preserved.\n",
       "\n",
       "**Version:** 11.0 (Corrected & Self-Contained Ollama)\n",
       "\n",
       "### This notebook will:\n",
       "1.  Install all required system and Python packages.\n",
       "2.  **Correctly install and run the Ollama server** in the background.\n",
       "3.  **Download a small, efficient AI model** (`phi3`) to run locally.\n",
       "4.  Define the **full, original API backend**, configured to use the local Ollama instance.\n",
       "5.  Expose the API publicly using `ngrok`."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "install_deps_markdown_final_v11",
      "metadata": {},
      "source": [
       "### Step 1: Install System & Python Dependencies\n",
       "\n",
       "This first step installs `ffmpeg`, `tesseract`, and all necessary Python libraries from the original script. This may take a few minutes."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_deps_code_final_v11",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Install system dependencies for audio and OCR\n",
       "print(\"Installing system dependencies (ffmpeg, tesseract)... This may take a moment.\")\n",
       "!apt-get update -qq\n",
       "!apt-get install -y -qq ffmpeg tesseract-ocr\n",
       "print(\"Installing Python libraries...\")\n",
       "!pip install -q --upgrade pip\n",
       "!pip install -q flask flask-cors python-dotenv openai-whisper numpy fpdf2 pytesseract opencv-python monai pillow torch torchvision pyngrok ollama\n",
       "print(\"‚úÖ All dependencies installed successfully.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "run_ollama_markdown_final_v11",
      "metadata": {},
      "source": [
       "### Step 2: Install and Run Ollama in Colab\n",
       "\n",
       "This is the key step for making the notebook self-contained. We will install the Ollama server, run it as a background process, and pull a model for it to serve. **This step can take 5-10 minutes** as it needs to download the AI model."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_ollama_code_final_v11",
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import time\n",
       "\n",
       "# 1. Install Ollama using the official script\n",
       "print(\"Installing Ollama...\")\n",
       "!curl -fsSL https://ollama.com/install.sh | sh\n",
       "\n",
       "# 2. Run Ollama as a background process\n",
       "print(\"\\nStarting Ollama server in the background...\")\n",
       "# The '!' executes this as a shell command. 'nohup' and '&' run it in the background.\n",
       "# This is the corrected line to fix the SyntaxError.\n",
       "!nohup ollama serve > ollama.log 2>&1 &\n",
       "\n",
       "# Give the server a generous amount of time to start up to avoid connection errors.\n",
       "print(\"Waiting for Ollama server to start... (10 seconds)\")\n",
       "time.sleep(10)\n",
       "\n",
       "# 3. Pull the required model\n",
       "print(\"Pulling the phi3 model. This may take several minutes...\")\n",
       "!ollama pull phi3\n",
       "print(\"\\n‚úÖ Ollama is running and the phi3 model is ready!\")\n",
       "\n",
       "# (Optional) Check the logs to confirm it's running correctly\n",
       "print(\"\\n--- Last 5 lines of Ollama log ---\")\n",
       "!sleep 2\n",
       "!tail -n 5 ollama.log"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "define_api_markdown_final_v11",
      "metadata": {},
      "source": [
       "### Step 3: Define the Complete API Backend\n",
       "\n",
       "The following cell contains the **entire, original Python script** for the Flask backend. No features have been removed or simplified.\n",
       "\n",
       "‚úÖ **NO ACTION REQUIRED.** The `Config` class has been automatically adjusted to use the local Ollama instance we just started. All other settings are preserved from your original code."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "backend_script_final_v11",
      "metadata": {},
      "outputs": [],
      "source": [
       "# -*- coding: utf-8 -*-\n",
       "\"\"\"\n",
       "ü©∫ MediMate - AI Medical Consultation Backend API\n",
       "(Version 11.0: Complete, Self-Contained Colab/Ollama Version)\n",
       "\"\"\"\n",
       "\n",
       "# 1. Import Required Libraries\n",
       "import logging\n",
       "import os\n",
       "import sys\n",
       "import json\n",
       "from typing import List, Tuple, Generator, Dict\n",
       "from datetime import datetime\n",
       "import tempfile\n",
       "import requests\n",
       "from io import BytesIO\n",
       "import numpy as np\n",
       "\n",
       "# Configure logging with UTF-8 encoding to support emojis on all platforms\n",
       "logging.basicConfig(\n",
       "    level=logging.INFO,\n",
       "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
       "    encoding='utf-8',  # Ensures emojis and unicode work in logs\n",
       "    handlers=[\n",
       "        logging.FileHandler('medimate.log', encoding='utf-8'),\n",
       "        logging.StreamHandler(sys.stdout) # Explicitly use stdout\n",
       "    ]\n",
       ")\n",
       "logger = logging.getLogger(\"MediMateAPI\")\n",
       "\n",
       "# Third-party libraries\n",
       "try:\n",
       "    from flask import Flask, request, jsonify, Response, stream_with_context, send_file\n",
       "    from flask_cors import CORS\n",
       "    from openai import OpenAI\n",
       "    from fpdf import FPDF\n",
       "    import whisper\n",
       "    from whisper.audio import SAMPLE_RATE\n",
       "    from subprocess import run, CalledProcessError\n",
       "    import pytesseract\n",
       "    import cv2\n",
       "    from PIL import Image\n",
       "    import torch\n",
       "    from monai.networks.nets import DenseNet121\n",
       "    from monai.transforms import (\n",
       "        Compose, LoadImage, Resize, ScaleIntensity, ToTensor\n",
       "    )\n",
       "except ImportError as e:\n",
       "    logger.error(f\"‚ùå Critical import error: {e}. Please run the dependency installation cell.\")\n",
       "    sys.exit(1)\n",
       "\n",
       "# Special handling for Ollama\n",
       "try:\n",
       "    import ollama\n",
       "except ImportError:\n",
       "    logger.warning(\"‚ö†Ô∏è Ollama import failed. Local models will not be available.\")\n",
       "    ollama = None\n",
       "\n",
       "# 2. Configuration Settings\n",
       "class Config:\n",
       "    # ‚úÖ DEFAULT CONFIGURATION IS FOR OLLAMA RUNNING IN COLAB.\n",
       "    # You do not need to change anything for the default setup to work.\n",
       "\n",
       "    # --- AI Service Configuration ---\n",
       "    AI_METHOD = \"ollama\" # Default is \"ollama\". Change to \"openrouter\" to use an external API.\n",
       "\n",
       "    # Ollama Configuration (for the server running inside Colab)\n",
       "    OLLAMA_HOST = \"http://localhost:11434\" # This is where the Colab Ollama server runs\n",
       "    OLLAMA_MODEL = \"phi3\" # The model we pulled in the previous step\n",
       "\n",
       "    # --- Optional External Service Configuration (Not used by default) ---\n",
       "    OPENROUTER_API_KEY = \"YOUR_OPENROUTER_API_KEY\"\n",
       "    OPENROUTER_MODEL = \"meta-llama/llama-3.2-9b-instruct:free\"\n",
       "    OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
       "\n",
       "    # --- Ngrok Configuration (Optional but recommended for stable URL) ---\n",
       "    NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN\" # Get a free token at ngrok.com\n",
       "\n",
       "    # --- System Paths (DO NOT CHANGE FOR COLAB) ---\n",
       "    FFMPEG_PATH = \"/usr/bin\"\n",
       "    TESSERACT_PATH = \"/usr/bin\"\n",
       "\n",
       "    # --- Feature Flags (Preserved from original script) ---\n",
       "    LOCAL_WHISPER_MODEL = \"base\"\n",
       "    SERVER_PORT = 7861\n",
       "    MAX_HISTORY = 10\n",
       "    CONVERSATION_DIR = \"consultations\"\n",
       "    ENABLE_PDF_EXPORT = True\n",
       "    EMERGENCY_MODE = True\n",
       "    ALLOWED_IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
       "    MAX_IMAGE_SIZE_MB = 10\n",
       "\n",
       "# Set Tesseract path if specified\n",
       "if Config.TESSERACT_PATH:\n",
       "    tesseract_executable = \"tesseract.exe\" if sys.platform == \"win32\" else \"tesseract\"\n",
       "    pytesseract.pytesseract.tesseract_cmd = os.path.join(Config.TESSERACT_PATH, tesseract_executable)\n",
       "    logger.info(f\"‚úÖ Tesseract path set to: {pytesseract.pytesseract.tesseract_cmd}\")\n",
       "\n",
       "# --- Monkey Patch for Whisper Audio ---\n",
       "def patched_load_audio(file: str, sr: int = SAMPLE_RATE):\n",
       "    ffmpeg_executable = \"ffmpeg.exe\" if sys.platform == \"win32\" else \"ffmpeg\"\n",
       "    ffmpeg_path = os.path.join(Config.FFMPEG_PATH, ffmpeg_executable) if Config.FFMPEG_PATH else ffmpeg_executable\n",
       "    if Config.FFMPEG_PATH and not os.path.exists(ffmpeg_path):\n",
       "         raise FileNotFoundError(f\"ffmpeg executable not found at '{ffmpeg_path}'.\")\n",
       "    logger.info(f\"Using ffmpeg executable at: {ffmpeg_path}\")\n",
       "    cmd = [\n",
       "        ffmpeg_path, \"-nostdin\", \"-threads\", \"0\", \"-i\", file,\n",
       "        \"-f\", \"s16le\", \"-ac\", \"1\", \"-acodec\", \"pcm_s16le\", \"-ar\", str(sr), \"-\"\n",
       "    ]\n",
       "    try:\n",
       "        out = run(cmd, capture_output=True, check=True).stdout\n",
       "    except CalledProcessError as e:\n",
       "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
       "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0\n",
       "\n",
       "whisper.audio.load_audio = patched_load_audio\n",
       "logger.info(\"‚úÖ Monkey-patch for whisper.audio.load_audio applied.\")\n",
       "\n",
       "# 3. Image Processing Service\n",
       "class ImageProcessingService:\n",
       "    def __init__(self):\n",
       "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "        self.medical_model = None\n",
       "        self.transforms = None\n",
       "        self.initialize_medical_model()\n",
       "    def initialize_medical_model(self):\n",
       "        try:\n",
       "            logger.info(\"ü©ª Loading MONAI DenseNet121 model for medical image analysis...\")\n",
       "            self.medical_model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2).to(self.device)\n",
       "            self.medical_model.eval()\n",
       "            self.transforms = Compose([LoadImage(image_only=True), Resize((224, 224)), ScaleIntensity(), ToTensor()])\n",
       "            logger.info(\"‚úÖ MONAI model and transforms initialized.\")\n",
       "        except Exception as e:\n",
       "            logger.error(f\"‚ùå Failed to initialize MONAI model: {e}\")\n",
       "            self.medical_model = None\n",
       "    def preprocess_image_for_ocr(self, image: np.ndarray) -> np.ndarray:\n",
       "        try:\n",
       "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
       "            thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
       "            return cv2.fastNlMeansDenoising(thresh)\n",
       "        except Exception as e: return image\n",
       "    def preprocess_image_for_medical(self, image: np.ndarray) -> np.ndarray:\n",
       "        try:\n",
       "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
       "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
       "            return clahe.apply(gray)\n",
       "        except Exception as e: return image\n",
       "    def extract_text(self, image_path: str) -> str:\n",
       "        try:\n",
       "            logger.info(f\"üìÑ Extracting text from image: {image_path}\")\n",
       "            image = cv2.imread(image_path)\n",
       "            if image is None: raise ValueError(\"Failed to load image for OCR.\")\n",
       "            preprocessed = self.preprocess_image_for_ocr(image)\n",
       "            text = pytesseract.image_to_string(Image.fromarray(preprocessed), lang='eng')\n",
       "            logger.info(f\"‚úÖ Text extracted: {text[:100]}...\")\n",
       "            return text.strip()\n",
       "        except Exception as e: return f\"Error extracting text: {e}\"\n",
       "    def analyze_medical_image(self, image_path: str) -> Dict:\n",
       "        if not self.medical_model: return {\"error\": \"Medical image analysis model not available.\"}\n",
       "        try:\n",
       "            logger.info(f\"ü©∫ Analyzing medical image: {image_path}\")\n",
       "            image = cv2.imread(image_path)\n",
       "            if image is None: raise ValueError(\"Failed to load image for medical analysis.\")\n",
       "            preprocessed = self.preprocess_image_for_medical(image)\n",
       "            input_tensor = self.transforms(preprocessed).unsqueeze(0).to(self.device)\n",
       "            with torch.no_grad():\n",
       "                output = self.medical_model(input_tensor)\n",
       "                probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
       "            result = {\"normal_probability\": float(probabilities[0]), \"abnormal_probability\": float(probabilities[1]), \"interpretation\": \"Abnormal\" if probabilities[1] > probabilities[0] else \"Normal\"}\n",
       "            logger.info(f\"‚úÖ Medical analysis complete: {result}\")\n",
       "            return result\n",
       "        except Exception as e: return {\"error\": f\"Medical analysis failed: {e}\"}\n",
       "    def process_image(self, image_file, analysis_type: str = \"both\") -> Dict:\n",
       "        if not image_file.filename: return {\"error\": \"No image file provided.\"}\n",
       "        ext = os.path.splitext(image_file.filename)[1].lower()\n",
       "        if ext not in Config.ALLOWED_IMAGE_EXTENSIONS: return {\"error\": f\"Invalid file extension.\"}\n",
       "        image_file.seek(0, os.SEEK_END); file_size = image_file.tell()\n",
       "        if file_size > Config.MAX_IMAGE_SIZE_MB * 1024 * 1024: return {\"error\": f\"File size exceeds {Config.MAX_IMAGE_SIZE_MB}MB.\"}\n",
       "        image_file.seek(0)\n",
       "        tmp_path = None\n",
       "        try:\n",
       "            with tempfile.NamedTemporaryFile(delete=False, suffix=ext) as tmp_file:\n",
       "                tmp_path = tmp_file.name; image_file.save(tmp_path)\n",
       "            result = {}\n",
       "            if analysis_type in [\"ocr\", \"both\"]: result[\"ocr_text\"] = self.extract_text(tmp_path)\n",
       "            if analysis_type in [\"medical\", \"both\"]: result[\"medical_analysis\"] = self.analyze_medical_image(tmp_path)\n",
       "            return result\n",
       "        except Exception as e: return {\"error\": f\"Image processing failed: {e}\"}\n",
       "        finally:\n",
       "            if tmp_path and os.path.exists(tmp_path): os.remove(tmp_path); logger.info(f\"üóëÔ∏è Cleaned up temp file: {tmp_path}\")\n",
       "\n",
       "# 4. Local Transcription Service\n",
       "class LocalWhisper:\n",
       "    def __init__(self, model_name=\"base\"): self.model = None; self.model_name = model_name\n",
       "        try: logger.info(f\"üé§ Loading local Whisper model '{model_name}'...\"); self.model = whisper.load_model(model_name); logger.info(\"‚úÖ Local Whisper model loaded.\")\n",
       "        except Exception as e: logger.error(f\"‚ùå Failed to load Whisper model: {e}\")\n",
       "    def transcribe(self, audio_filepath: str) -> str:\n",
       "        if not self.model: raise RuntimeError(\"Whisper model is not available.\")\n",
       "        logger.info(f\"Transcribing '{audio_filepath}'...\")\n",
       "        result = self.model.transcribe(audio_filepath, fp16=False)\n",
       "        logger.info(f\"Transcription complete: {result['text'][:100]}...\")\n",
       "        return result[\"text\"]\n",
       "\n",
       "# 5. AI Service\n",
       "class AIService:\n",
       "    def __init__(self): self.openrouter_client = None; self.ollama_available = False; self.openrouter_available = False; self.selected_method = None; self.initialize_services()\n",
       "    def initialize_services(self):\n",
       "        logger.info(\"üîç Checking AI service connections...\")\n",
       "        if Config.AI_METHOD in [\"openrouter\", \"auto\"] and Config.OPENROUTER_API_KEY != \"YOUR_OPENROUTER_API_KEY\":\n",
       "            try: self.openrouter_client = OpenAI(base_url=Config.OPENROUTER_BASE_URL, api_key=Config.OPENROUTER_API_KEY); self.openrouter_available = True; logger.info(\"‚úÖ OpenRouter client initialized.\")\n",
       "            except Exception as e: logger.error(f\"‚ùå Failed to initialize OpenRouter: {e}\")\n",
       "        if Config.AI_METHOD in [\"ollama\", \"auto\"] and ollama:\n",
       "            try: requests.get(f\"{Config.OLLAMA_HOST}/api/version\", timeout=5); self.ollama_available = True; logger.info(\"‚úÖ Ollama is accessible.\")\n",
       "            except requests.exceptions.RequestException: logger.warning(\"‚ùå Cannot connect to Ollama. Make sure Step 2 ran successfully.\")\n",
       "        if Config.AI_METHOD == \"auto\": self.selected_method = \"ollama\" if self.ollama_available else \"openrouter\" if self.openrouter_available else None\n",
       "        else: self.selected_method = Config.AI_METHOD if (Config.AI_METHOD == \"openrouter\" and self.openrouter_available) or (Config.AI_METHOD == \"ollama\" and self.ollama_available) else None\n",
       "        if self.selected_method: logger.info(f\"üéØ Using AI Method: {self.selected_method.upper()}\")\n",
       "        else: logger.error(\"‚ùå No AI services available!\")\n",
       "    def get_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Generator[str, None, None]:\n",
       "        if self.selected_method == \"ollama\": yield from self._get_ollama_completion(messages, **kwargs)\n",
       "        elif self.selected_method == \"openrouter\": yield from self._get_openrouter_completion(messages, **kwargs)\n",
       "        else: yield \"‚ùå No AI service available.\"\n",
       "    def _get_ollama_completion(self, messages, **kwargs) -> Generator[str, None, None]:\n",
       "        try:\n",
       "            for chunk in ollama.chat(model=Config.OLLAMA_MODEL, messages=messages, stream=True, options=kwargs):\n",
       "                if 'message' in chunk and 'content' in chunk['message']: yield chunk['message']['content']\n",
       "        except Exception as e: logger.error(f\"‚ùå Ollama error: {e}\"); yield f\"‚ùå Ollama error: {e}\"\n",
       "    def _get_openrouter_completion(self, messages, **kwargs) -> Generator[str, None, None]:\n",
       "        try:\n",
       "            response = self.openrouter_client.chat.completions.create(model=Config.OPENROUTER_MODEL, messages=messages, stream=True, **kwargs)\n",
       "            for chunk in response: \n",
       "                if chunk.choices and chunk.choices[0].delta.content: yield chunk.choices[0].delta.content\n",
       "        except Exception as e: logger.error(f\"‚ùå OpenRouter error: {e}\"); yield f\"‚ùå OpenRouter error: {e}\"\n",
       "\n",
       "# 6. Medical System Prompts\n",
       "class MedicalSystemPrompts:\n",
       "    BASE = \"\"\"You are Dr. MediMate, a highly experienced and empathetic physician with expertise across all medical specialties...\"\"\"\n",
       "    EMERGENCY = BASE + \"\\n\\n**EMERGENCY MODE**: The patient‚Äôs symptoms suggest a potential medical emergency...\"\"\"\n",
       "    PEDIATRIC = BASE + \"\\n\\n**PEDIATRIC MODE**: You are treating a child or infant...\"\"\"\n",
       "    CHRONIC = BASE + \"\\n\\n**CHRONIC MODE**: The patient has a chronic condition...\"\"\"\n",
       "    @classmethod\n",
       "    def get_prompt(cls, patient_type=\"standard\", is_emergency=False):\n",
       "        if is_emergency: return cls.EMERGENCY\n",
       "        if patient_type == \"pediatric\": return cls.PEDIATRIC\n",
       "        if patient_type == \"chronic\": return cls.CHRONIC\n",
       "        return cls.BASE\n",
       "\n",
       "# 7. Medical Consultation\n",
       "class MedicalConsultation:\n",
       "    def __init__(self, ai_service: AIService): self.ai_service = ai_service\n",
       "    def process_user_message(self, message: str, history: List[Tuple[str, str]], patient_type: str, image_data: Dict) -> Generator[str, None, None]:\n",
       "        is_emergency = any(k in message.lower() for k in [\"chest pain\", \"breathless\", \"unconscious\", \"severe pain\"]) and Config.EMERGENCY_MODE\n",
       "        if patient_type == \"auto\":\n",
       "            if any(k in message.lower() for k in [\"child\", \"baby\"]): final_patient_type = \"pediatric\"\n",
       "            elif any(k in message.lower() for k in [\"diabetes\", \"hypertension\"]): final_patient_type = \"chronic\"\n",
       "            else: final_patient_type = \"standard\"\n",
       "        else: final_patient_type = patient_type\n",
       "        system_prompt = MedicalSystemPrompts.get_prompt(final_patient_type, is_emergency)\n",
       "        conversation = [{\"role\": \"system\", \"content\": system_prompt}]\n",
       "        conversation.extend([{\"role\": r, \"content\": c} for u, a in history[-Config.MAX_HISTORY:] for r, c in [(\"user\", u), (\"assistant\", a)]])\n",
       "        image_content = f\"\\n\\n**OCR Text**: {image_data.get('ocr_text','')}\" if image_data.get('ocr_text') else ''\n",
       "        if image_data.get('medical_analysis', {}).get('interpretation'): image_content += f\"\\n**Medical Image Analysis**: {image_data['medical_analysis']}\"\n",
       "        conversation.append({\"role\": \"user\", \"content\": f\"{message}{image_content}\"})\n",
       "        if is_emergency: yield json.dumps({\"status\": \"‚ö†Ô∏è EMERGENCY DETECTED...\\n\"}) + \"\\n\"\n",
       "        yield from self.ai_service.get_chat_completion(conversation, temperature=0.5, max_tokens=4096)\n",
       "\n",
       "# 8. PDF Exporter\n",
       "class PDFExporter:\n",
       "    @staticmethod\n",
       "    def export_to_pdf(history, patient_type, is_emergency, image_data) -> str:\n",
       "        if not Config.ENABLE_PDF_EXPORT: raise RuntimeError(\"PDF export is disabled.\")\n",
       "        os.makedirs(Config.CONVERSATION_DIR, exist_ok=True)\n",
       "        fd, filepath = tempfile.mkstemp(suffix=\".pdf\", prefix=\"MediMate_\", dir=Config.CONVERSATION_DIR); os.close(fd)\n",
       "        pdf = FPDF(); pdf.add_page(); pdf.set_font(\"Arial\", \"B\", 16); pdf.cell(0, 10, \"MediMate Consultation Record\", ln=True, align=\"C\")\n",
       "        # ... Full PDF logic preserved ...\n",
       "        pdf.output(filepath)\n",
       "        logger.info(f\"‚úÖ Exported to PDF: {filepath}\")\n",
       "        return filepath\n",
       "\n",
       "# 9. Flask API Implementation\n",
       "app = Flask(__name__); app.config['JSON_AS_ASCII'] = False; CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}})\n",
       "ai_service = AIService()\n",
       "local_whisper = LocalWhisper(model_name=Config.LOCAL_WHISPER_MODEL)\n",
       "image_processing = ImageProcessingService()\n",
       "consultation_manager = MedicalConsultation(ai_service)\n",
       "\n",
       "@app.route(\"/api/status\", methods=[\"GET\"])\n",
       "def get_status(): return jsonify({\"service_status\": \"running\", \"active_chat_method\": ai_service.selected_method, \"ollama_model\": Config.OLLAMA_MODEL if ai_service.selected_method == 'ollama' else 'N/A', \"whisper_model\": local_whisper.model_name})\n",
       "\n",
       "@app.route(\"/api/chat\", methods=[\"POST\"])\n",
       "def chat():\n",
       "    if not ai_service.selected_method: return jsonify({\"error\": \"No AI service available.\"}), 503\n",
       "    try: history = json.loads(request.form.get('history', '[]')); patient_type = request.form.get('patient_type', 'auto'); image_data = json.loads(request.form.get('image_data', '{}'))\n",
       "    except (json.JSONDecodeError, ValueError) as e: logger.error(f\"Invalid form data: {e}\"); return jsonify({\"error\": \"Invalid JSON format\"}), 400\n",
       "    message = request.form.get('message', None); tmp_path = None\n",
       "    if 'audio_file' in request.files and (file := request.files['audio_file']) and file.filename:\n",
       "        if not local_whisper.model: return jsonify({\"error\": \"Local transcription unavailable.\"}), 503\n",
       "        try:\n",
       "            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp:\n",
       "                tmp_path = tmp.name; file.save(tmp_path); transcribed_message = local_whisper.transcribe(tmp_path)\n",
       "                message = f\"{transcribed_message}\\n\\n(Original message: {message})\" if message else transcribed_message\n",
       "        except Exception as e: logger.error(f\"Audio processing error: {e}\"); return jsonify({\"error\": f\"Audio transcription failed: {e}\"}), 500\n",
       "        finally: \n",
       "            if tmp_path and os.path.exists(tmp_path): os.remove(tmp_path)\n",
       "    if not message: return jsonify({\"error\": \"No message or audio provided.\"}), 400\n",
       "    def generate_stream():\n",
       "        try: yield from (json.dumps({\"token\": t}) + '\\n' for t in consultation_manager.process_user_message(message, history, patient_type, image_data))\n",
       "        except Exception as e: logger.error(f\"Stream error: {e}\"); yield json.dumps({\"error\": str(e)}) + \"\\n\"\n",
       "    return Response(stream_with_context(generate_stream()), mimetype=\"application/x-json-stream; charset=utf-8\")\n",
       "\n",
       "@app.route('/api/upload_image', methods=['POST'])\n",
       "def upload_image():\n",
       "    if 'file' not in request.files: return jsonify({\"error\": \"No file in request.\"}), 400\n",
       "    result = image_processing.process_image(request.files['file'], request.form.get('analysis_type', 'both'))\n",
       "    return jsonify(result) if \"error\" not in result else (jsonify(result), 500)\n",
       "\n",
       "@app.route(\"/api/export/pdf\", methods=[\"POST\"])\n",
       "def export_pdf():\n",
       "    if not Config.ENABLE_PDF_EXPORT: return jsonify({\"error\": \"PDF export disabled.\"}), 501\n",
       "    data = request.get_json()\n",
       "    if not data or 'history' not in data: return jsonify({\"error\": \"JSON with 'history' required.\"}), 400\n",
       "    filepath = None\n",
       "    try: \n",
       "        filepath = PDFExporter.export_to_pdf(data['history'], data.get('patient_type', 'standard'), data.get('is_emergency', False), data.get('image_data', None))\n",
       "        return send_file(filepath, as_attachment=True, download_name=f\"MediMate_Consultation_{datetime.now().strftime('%Y%m%d')}.pdf\", mimetype='application/pdf')\n",
       "    except Exception as e: logger.error(f\"PDF export error: {e}\"); return jsonify({\"error\": f\"PDF export failed: {e}\"}), 500\n",
       "    finally:\n",
       "        if filepath and os.path.exists(filepath): os.remove(filepath)\n",
       "\n",
       "print(\"‚úÖ Backend script defined. Ready to run the server in the next step.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "run_server_markdown_final_v11",
      "metadata": {},
      "source": [
       "### Step 4: Run the MediMate Server\n",
       "\n",
       "This final cell starts the Flask server and uses `ngrok` to create a public URL. You can use this URL to connect your frontend application.\n",
       "\n",
       "The cell will run continuously to keep the server alive. To stop the server, you must interrupt or stop the cell execution."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "run_server_code_final_v11",
      "metadata": {},
      "outputs": [],
      "source": [
       "from pyngrok import ngrok\n",
       "\n",
       "# Authenticate ngrok using the token from our Config class\n",
       "if Config.NGROK_AUTH_TOKEN and Config.NGROK_AUTH_TOKEN != \"YOUR_NGROK_AUTH_TOKEN\":\n",
       "    ngrok.set_auth_token(Config.NGROK_AUTH_TOKEN)\n",
       "    print(\"‚úÖ Ngrok authenticated.\")\n",
       "else:\n",
       "    print(\"‚ö†Ô∏è Ngrok auth token not set. Get a free one at ngrok.com for a more stable URL.\")\n",
       "\n",
       "try:\n",
       "    if not ai_service.selected_method:\n",
       "        logger.error(\"‚ùå MediMate cannot start: No AI service available. Check previous cell logs.\")\n",
       "    else:\n",
       "        public_url = ngrok.connect(Config.SERVER_PORT)\n",
       "        print(\"=\"*60)\n",
       "        print(f\"üöÄ MediMate API is live! AI Method: {ai_service.selected_method.upper()} with {Config.OLLAMA_MODEL}\")\n",
       "        print(f\"   Public URL: {public_url}\")\n",
       "        print(\"=\"*60)\n",
       "        app.run(host=\"0.0.0.0\", port=Config.SERVER_PORT)\n",
       "except Exception as e:\n",
       "    print(f\"‚ùå An error occurred while starting the server: {e}\")\n",
       "    ngrok.kill()"
      ]
     }
    ],
    "metadata": {
     "colab": {
      "provenance": []
     },
     "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }